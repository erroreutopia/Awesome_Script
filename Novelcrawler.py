#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°è¯´çˆ¬è™«ï¼šæ”¯æŒæ··åˆç« èŠ‚æ ¼å¼ + è‡ªå®šä¹‰è§„åˆ™ + å°é¢ + è°ƒè¯•HTML + å±è”½é‡å¤ç½®é¡¶ç« 
ä½œè€…ï¼šæ ¹æ®ç”¨æˆ·éœ€æ±‚æŒç»­ä¼˜åŒ–
"""

import os
import re
import time
import traceback
import requests
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from ebooklib import epub

# === é…ç½® ===
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
NOVEL_DIR = os.path.join(SCRIPT_DIR, "novel")
os.makedirs(NOVEL_DIR, exist_ok=True)

# Chromium è·¯å¾„ï¼ˆLinux å¸¸è§è·¯å¾„ï¼‰
CHROMIUM_PATH = "/usr/bin/chromium"

chrome_options = Options()
chrome_options.binary_location = CHROMIUM_PATH
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--disable-images")
chrome_options.add_argument("--log-level=3")

print("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
driver = webdriver.Chrome(options=chrome_options)

def sanitize_filename(name):
    return re.sub(r'[\\/:*?"<>|]', '_', name).strip() or "novel"

def load_rules_from_file():
    rules_path = os.path.join(SCRIPT_DIR, "rules.txt")
    if os.path.exists(rules_path):
        with open(rules_path, "r", encoding="utf-8") as f:
            rules = [line.strip() for line in f if line.strip()]
        print(f"âœ… ä» rules.txt åŠ è½½ {len(rules)} æ¡è§„åˆ™")
        return rules
    else:
        print("âš ï¸ rules.txt ä¸å­˜åœ¨ï¼Œè·³è¿‡æ–‡ä»¶åŠ è½½")
        return []

def get_user_rules():
    use_custom = input("æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰ç« èŠ‚åŒ¹é…è§„åˆ™ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
    if use_custom == 'y':
        method = input("è¾“å…¥æ–¹å¼ï¼š1=äº¤äº’è¾“å…¥ï¼Œ2=ä» rules.txt åŠ è½½ (é»˜è®¤ 2): ").strip()
        if method == '1':
            rule = input("è¯·è¾“å…¥ç« èŠ‚æ ‡é¢˜æ­£åˆ™è¡¨è¾¾å¼: ").strip()
            if rule:
                return [rule]
            else:
                print("æœªè¾“å…¥è§„åˆ™ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™")
        elif method == '2' or not method:
            rules = load_rules_from_file()
            if rules:
                return rules
            else:
                print("rules.txt ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™")
    return [
        r'\d+\.\s*(ç¬¬?\d+ç« ?)',
        r'ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç« ',
        r'ç¬¬\d+ç« ',
        r'^\d{3,}\s+.+',
        r'\d+\s*[-â€“â€”]\s*.*',
        r'Chapter\s*\d+',
        r'ã€[^ã€‘]*\s*\d+ã€‘',
        r'ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+[å›èŠ‚ç¯‡å·]'
    ]

def extract_chapter_links(toc_url, rules):
    print("æ­£åœ¨åŠ è½½ç›®å½•é¡µ...")
    driver.get(toc_url)
    
    # è‡ªåŠ¨æ»šåŠ¨åŠ è½½å…¨éƒ¨ç« èŠ‚
    for _ in range(8):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.2)

    soup = BeautifulSoup(driver.page_source, 'lxml')
    all_a_tags = soup.find_all('a', href=True)
    compiled_rules = [re.compile(rule, re.IGNORECASE) for rule in rules]
    seen_hrefs = set()
    final_links = []

    for a in all_a_tags:
        href = a['href']
        if href.startswith('/'):
            full_href = urljoin(toc_url, href)
        elif not href.startswith(('http://', 'https://')):
            full_href = toc_url.rstrip('/') + '/' + href.lstrip('/')
        else:
            full_href = href

        if full_href in seen_hrefs:
            continue

        text = a.get_text(strip=True)
        if any(pattern.search(text) for pattern in compiled_rules):
            final_links.append((text, full_href))
            seen_hrefs.add(full_href)

    print(f"âœ… åˆå§‹åŒ¹é… {len(final_links)} ç« ï¼ˆæŒ‰é¡µé¢åŸå§‹é¡ºåºï¼‰")

    # å±è”½å‰ N ç« ï¼ˆé˜²ç½®é¡¶é‡å¤ï¼‰
    skip_n = 0
    if len(final_links) > 10:
        try:
            skip_input = input("æ˜¯å¦å±è”½ç›®å½•é¡µå‰ N ç« ï¼ˆé˜²ç½®é¡¶é‡å¤ï¼‰ï¼Ÿ(ç›´æ¥å›è½¦=0): ").strip()
            skip_n = int(skip_input) if skip_input.isdigit() else 0
        except:
            skip_n = 0

    if skip_n > 0:
        print(f"âš ï¸ å±è”½å‰ {skip_n} ç« ")
        final_links = final_links[skip_n:]

    print(f"ğŸ“Œ æœ€ç»ˆä¿ç•™ {len(final_links)} ç« ")

    # é¢„è§ˆ
    print("å‰5ç« é¢„è§ˆ:")
    for i, (title, _) in enumerate(final_links[:5]):
        print(f"  {i+1}. {title}")
    if len(final_links) > 10:
        print("å5ç« é¢„è§ˆ:")
        for i, (title, _) in enumerate(final_links[-5:], start=len(final_links)-4):
            print(f"  {i}. {title}")

    confirm = input("ç« èŠ‚é¡ºåºå’Œå†…å®¹æ­£ç¡®å—ï¼Ÿ(y/nï¼Œé»˜è®¤ y): ").strip().lower()
    if confirm in ('', 'y', 'yes'):
        return final_links
    else:
        print("âŒ ç”¨æˆ·å¦å†³")
        return []

def get_chapter_content(ch_url):
    driver.get(ch_url)
    time.sleep(1.2)
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'lxml')

    debug_dir = os.path.join(NOVEL_DIR, "debug_pages")
    os.makedirs(debug_dir, exist_ok=True)
    safe_name = re.sub(r'[\\/:*?"<>|]', '_', ch_url.replace('https://', '').replace('http://', ''))[:100]
    with open(os.path.join(debug_dir, f"{safe_name}.html"), "w", encoding="utf-8") as f:
        f.write(page_source)

    selectors = [
        '#content', '.content', '#chapter-content', '.chapter-content',
        '.read-content', '#read-content', '.txt', '.text', '.article-content',
        'article', 'main', '.post-content', '.entry-content', '.bookcontent'
    ]
    for sel in selectors:
        elem = soup.select_one(sel)
        if elem:
            print(f"  âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{sel}' æå–æˆåŠŸ")
            return str(elem)

    for elem in soup.find_all(string=re.compile(r'.', re.DOTALL)):
        if elem.parent and elem.parent.name in ['h1', 'h2', 'h3', 'h4']:
            txt = elem.strip()
            if len(txt) > 5 and any(kw in txt for kw in ['ç¬¬', 'ç« ', 'å›', 'èŠ‚', 'Chapter', 'Episode', 'ã€']):
                parent = elem.parent
                parts = []
                for sibling in parent.next_siblings:
                    if hasattr(sibling, 'get_text'):
                        t = sibling.get_text(strip=True)
                        if len(t) > 10:
                            parts.append(str(sibling))
                if parts:
                    print("  âœ… é€šè¿‡æ ‡é¢˜åå…„å¼ŸèŠ‚ç‚¹æå–æˆåŠŸ")
                    return ''.join(parts)

    paragraphs = soup.find_all('p')
    long_ps = [str(p) for p in paragraphs if len(p.get_text(strip=True)) > 20]
    if len(long_ps) > 2:
        print("  âš ï¸ é€€åŒ–ï¼šä½¿ç”¨ <p> æ ‡ç­¾ç»„åˆ")
        return ''.join(long_ps)

    print("  âŒ æ­£æ–‡æå–å¤±è´¥ï¼åŸå§‹é¡µé¢å·²ä¿å­˜åˆ° debug_pages/")
    return "<p>æ­£æ–‡æå–å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ debug_pages/ ä¸­çš„ HTML æ–‡ä»¶ã€‚</p>"

def create_epub(title, author, cover_path_or_url, chapters, output_dir):
    safe_title = sanitize_filename(title or "å°è¯´")
    book = epub.EpubBook()
    book.set_identifier(f'novel_{abs(hash(safe_title)) % (10**8)}')
    book.set_title(title or "å°è¯´")
    book.set_language('zh')
    book.add_author(author or "åŒ¿å")

    if cover_path_or_url:
        try:
            if cover_path_or_url.startswith(('http://', 'https://')):
                print("æ­£åœ¨ä¸‹è½½å°é¢...")
                resp = requests.get(cover_path_or_url, timeout=10)
                resp.raise_for_status()
                cover_data = resp.content
                content_type = resp.headers.get('content-type', 'image/jpeg')
                if 'png' in content_type:
                    cover_ext = "png"
                elif 'gif' in content_type:
                    cover_ext = "gif"
                else:
                    cover_ext = "jpg"
            else:
                if not os.path.exists(cover_path_or_url):
                    print(f"âš ï¸ å°é¢æ–‡ä»¶ä¸å­˜åœ¨: {cover_path_or_url}ï¼Œè·³è¿‡å°é¢")
                    cover_data = None
                else:
                    with open(cover_path_or_url, "rb") as f:
                        cover_data = f.read()
                    _, ext = os.path.splitext(cover_path_or_url)
                    cover_ext = ext.lower().lstrip('.')

            if 'cover_data' in locals() and cover_data is not None:
                cover_file_name = f"cover.{cover_ext}"
                book.set_cover(cover_file_name, cover_data)

        except Exception as e:
            print(f"âš ï¸ å°é¢åŠ è½½å¤±è´¥: {e}")

    spine = ['nav']
    toc = []
    for i, (ch_title, ch_html) in enumerate(chapters):
        chapter = epub.EpubHtml(
            title=ch_title,
            file_name=f'chap_{i+1:04}.xhtml',
            lang='zh'
        )
        chapter.content = f'<h1>{ch_title}</h1>' + (ch_html or "<p>ç©ºå†…å®¹</p>")
        book.add_item(chapter)
        toc.append(chapter)
        spine.append(chapter)

    book.toc = tuple(toc)
    book.add_item(epub.EpubNcx())
    book.add_item(epub.EpubNav())
    book.spine = spine

    epub_path = os.path.join(output_dir, f"{safe_title}.epub")
    epub.write_epub(epub_path, book)
    return epub_path

# === ä¸»ç¨‹åº ===
if __name__ == '__main__':
    try:
        TOC_URL = input("è¯·è¾“å…¥å°è¯´ç›®å½•é¡µå®Œæ•´é“¾æ¥ï¼ˆä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰: ").strip()
        if not TOC_URL.startswith(('http://', 'https://')):
            print("âŒ é“¾æ¥æ ¼å¼é”™è¯¯ï¼")
            exit(1)

        rules = get_user_rules()
        chapter_list = extract_chapter_links(TOC_URL, rules)
        if not chapter_list:
            print("âŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆç« èŠ‚ï¼Œè¯·æ£€æŸ¥è§„åˆ™æˆ–ç½‘é¡µç»“æ„ã€‚")
            exit(1)

        driver.get(TOC_URL)
        time.sleep(1)
        try:
            default_title = driver.title.replace('ç›®å½•', '').replace('å°è¯´', '').replace('æœ€æ–°ç« èŠ‚', '').strip()
        except:
            default_title = "æˆ‘çš„å°è¯´"

        print("\n--- EPUB å…ƒæ•°æ®è®¾ç½®ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼ï¼‰ ---")
        custom_title = input(f"ä¹¦åï¼ˆé»˜è®¤: {default_title}ï¼‰: ").strip() or default_title
        custom_author = input("ä½œè€…ï¼ˆé»˜è®¤: åŒ¿åï¼‰: ").strip() or "åŒ¿å"
        cover_input = input("å°é¢ï¼ˆæœ¬åœ°è·¯å¾„å¦‚ cover.jpgï¼Œæˆ–ç½‘ç»œé“¾æ¥ï¼Œç•™ç©ºåˆ™æ— å°é¢ï¼‰: ").strip()

        chapters_content = []
        for i, (ch_title, ch_url) in enumerate(chapter_list):
            print(f"[{i+1}/{len(chapter_list)}] æ­£åœ¨ä¸‹è½½ï¼š{ch_title}")
            try:
                content = get_chapter_content(ch_url)
                chapters_content.append((ch_title, content))
                print(f"    â†’ æå–åˆ° {len(content)} å­—ç¬¦")
            except Exception as e:
                print(f"    âŒ é”™è¯¯: {e}")
                traceback.print_exc()
                chapters_content.append((ch_title, "<p>åŠ è½½å¼‚å¸¸</p>"))

        print("æ­£åœ¨ç”Ÿæˆ EPUB...")
        epub_file = create_epub(
            title=custom_title,
            author=custom_author,
            cover_path_or_url=cover_input,
            chapters=chapters_content,
            output_dir=NOVEL_DIR
        )
        print(f"ğŸ‰ å®Œæˆï¼EPUB å·²ä¿å­˜è‡³ï¼š{epub_file}")
        print(f"ğŸ“„ è°ƒè¯• HTML åœ¨ï¼š{os.path.join(NOVEL_DIR, 'debug_pages')}")

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"âŒ ç¨‹åºå´©æºƒ: {e}")
        traceback.print_exc()
    finally:
        print("æ­£åœ¨å…³é—­æµè§ˆå™¨...")
        driver.quit()